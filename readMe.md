# TransLogix Analytics Azure Data Engineering Solution

## Overview

TransLogix Analytics is a data engineering solution designed to leverage Azure services for scalable, secure, and efficient analytics. This project enables data ingestion, transformation, storage, and visualization for logistics and transportation data.

## Features

- **Azure Data Factory** for orchestrating data pipelines
- **Azure Data Lake Storage** for scalable data storage
- **Azure Databricks** for data transformation and analytics
- **Azure Synapse Analytics** for data warehousing
- **Power BI** integration for interactive dashboards

## Architecture

```mermaid
graph TD
    A[Data Sources] --> B[Azure Data Factory]
    B --> C[Azure Data Lake Storage]
    C --> D[Azure Databricks]
    D --> E[Azure Synapse Analytics]
    E --> F[Power BI]
```

## ğŸ“ Folder Structure

logistics-analytics-platform/
â”‚
â”œâ”€â”€ data/ # Dummy CSVs (drivers, vendors, routes, shipments)
â”œâ”€â”€ notebooks/ # Databricks Notebooks for each layer
â”œâ”€â”€ adf_pipelines/ # ADF JSON definitions
â”œâ”€â”€ powerbi/ # Power BI screenshots or .pbix files
â”œâ”€â”€ architecture/ # Architecture diagram image
â”œâ”€â”€ README.md # This file
â””â”€â”€ .gitignore


## ğŸ“Š Power BI Dashboard Features

- âœ… KPI Cards: Total Shipments, Avg Delay, On-Time %
- ğŸ“Š Vendor performance bar charts
- ğŸ“ˆ Monthly delivery trend lines
- ğŸ—ºï¸ Route-level delay matrix (simulated map)
- ğŸ¯ Filters: Vendor, Route Type, Origin, Destination

---

## ğŸ“Œ Pipeline Stages

### ğŸ”¹ Bronze Layer (Raw Ingestion)
- Raw files: `drivers.csv`, `vendors.csv`, `routes.csv`, `shipments.csv`
- Ingested using **ADF Copy Activity** with **ForEach loop**
- Stored in Azure Data Lake under the `bronze/` container

### ğŸ”¸ Silver Layer (Cleansed)
- Field renaming and schema standardization
- Converted timestamps and metrics (e.g., delay in minutes)
- Stored as partitioned Parquet files in `silver/` container

### ğŸŸ¡ Gold Layer (Aggregated)
- Enriched metrics like:
  - On-Time %
  - Delay by route
  - Vendor KPIs
  - Monthly trends
- Written to `gold/` container, ready for Power BI

---

## ğŸ“¤ How to Reproduce

1. Clone this repo
2. Upload `/data/` CSVs to Azure Data Lake Gen2 (`raw` container)
3. Import and run ADF pipelines from `/adf_pipelines/`
4. Execute transformation notebooks in `/notebooks/` inside Databricks
5. Open Power BI file from `/powerbi/` and connect to `gold` container


## Prerequisites

- Azure Subscription
- Access to Azure Portal
- Power BI account

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.
